{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Markov Projection \u00e2\u20ac\u201d Interactive Notebook\n",
    "Configure, run, and inspect the projection pipeline without changing the library code.\n",
    "\n",
    "**Usage tips**\n",
    "- Edit the config overrides in the cells below instead of hard-coding in modules.\n",
    "- Choose `parquet` or `oracle` as the data source.\n",
    "- Optional: generate a synthetic parquet sample if you do not have input data handy.\n",
    "- Outputs (CSV + Parquet) are written to the directory you set in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup: locate project root that contains config.py and src/\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(start: Path, marker: str = \"config.py\", max_depth: int = 7) -> Path:\n",
    "    \"\"\"Search current and parent directories (and their risk_markov_projection child) for marker and src/.\"\"\"\n",
    "    candidates = []\n",
    "    current = start\n",
    "    for _ in range(max_depth):\n",
    "        candidates.append(current)\n",
    "        candidates.append(current / \"risk_markov_projection\")\n",
    "        current = current.parent\n",
    "    for cand in candidates:\n",
    "        if (cand / marker).exists() and (cand / \"src\").exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError(f\"Could not find project root from {start}\")\n",
    "\n",
    "CWD = Path.cwd().resolve()\n",
    "ROOT = find_project_root(CWD)\n",
    "SRC = ROOT / \"src\"\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "print(\"Detected project root:\", ROOT)\n",
    "print(\"Src path added:\", SRC)\n",
    "print(\"Working directory:\", CWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ff544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (force-load project config to avoid name conflicts)\n",
    "import importlib\n",
    "import importlib.util\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Explicitly load config.py from project root and register in sys.modules\n",
    "config_path = ROOT / \"config.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"config\", config_path)\n",
    "config = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"config\"] = config\n",
    "assert spec.loader is not None\n",
    "spec.loader.exec_module(config)\n",
    "\n",
    "from src.data.data_loader import load_raw_data\n",
    "from src.data.schema import default_schema\n",
    "from src.data.validators import validate_input\n",
    "from src.pipelines.run_projection import run\n",
    "from src.utils.logger import get_logger\n",
    "from src.utils.export_excel import export_projection_excel\n",
    "from src.utils.cohort_report import export_cohort_del30_report\n",
    "from src.utils.cohort_report import export_cohort_del30_excel_combined\n",
    "from src.utils.cohort_report import export_cohort_del30_excel_split\n",
    "from src.utils.lifecycle_report import build_lifecycle_for_report\n",
    "from src.utils.lifecycle_report import export_lifecycle_all_products_one_file_extended\n",
    "\n",
    "logger = get_logger(\"notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b76d60",
   "metadata": {},
   "source": [
    "## Configure runtime parameters\n",
    "Adjust the variables below to control data source, thresholds, and output paths at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose data source and paths\n",
    "DATA_SOURCE = \"oracle\"  # options: \"parquet\" or \"oracle\"\n",
    "PARQUET_PATH = Path(r\"C:/Users/MAFC4709/Python_work/Projection/data/parquet/POS\")\n",
    "GENERATE_SYNTHETIC = False  # set True to create a sample parquet if you have no data\n",
    "\n",
    "# Oracle SQL overrides (used only when DATA_SOURCE == \"oracle\")\n",
    "import textwrap\n",
    "\n",
    "config.ORACLE_CONFIG[\"sql\"] = textwrap.dedent(\"\"\"\n",
    "    SELECT\n",
    "      CUTOFF_DATE,\n",
    "      AGREEMENT_ID,\n",
    "      DISBURSAL_DATE,\n",
    "      DISBURSAL_AMOUNT,\n",
    "      DPD_EOM,\n",
    "      RISK_BUCKET,\n",
    "      PRINCIPLE_OUTSTANDING,\n",
    "      STATUS,\n",
    "      MOB,\n",
    "      MAFC_SUB_CATEGORY AS PRODUCT_TYPE,\n",
    "      NPA_STAGEID,\n",
    "      DPD_GROUP,\n",
    "      STATE_MODEL,\n",
    "      MSCORE_GROUP AS RISK_SCORE,\n",
    "      SALE_CHANNEL\n",
    "    FROM RISK.TV_MARKOV_POS_RR a\n",
    "    WHERE a.CUTOFF_DATE <= DATE '2025-10-01'\n",
    "      AND a.CUTOFF_DATE >= DATE '2024-01-01'\n",
    "\"\"\")\n",
    "\n",
    "config.ORACLE_CONFIG[\"params\"] = config.ORACLE_CONFIG.get(\"params\", {})\n",
    "config.ORACLE_CONFIG[\"sql_dir\"] = config.ORACLE_CONFIG.get(\"sql_dir\", \"sql\")\n",
    "\n",
    "# Thresholds and other runtime knobs\n",
    "config.MIN_OBS = 50\n",
    "config.MIN_EAD = 100.0\n",
    "config.MAX_MOB = 24\n",
    "config.CALIBRATION[\"enabled\"] = True\n",
    "\n",
    "# Output overrides (keeps notebook outputs separate)\n",
    "config.OUTPUT[\"dir\"] = ROOT / \"outputs\" / \"notebook\"\n",
    "config.OUTPUT[\"csv_name\"] = \"projection_notebook.csv\"\n",
    "config.OUTPUT[\"parquet_name\"] = \"projection_notebook.parquet\"\n",
    "\n",
    "# Apply data source choice\n",
    "config.DATA_SOURCE = DATA_SOURCE\n",
    "config.PARQUET_PATH = PARQUET_PATH\n",
    "\n",
    "config.DATA_SOURCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching checkpoints\n",
    "Set cache paths and toggles to avoid rerunning heavy steps. Clear cache if you change data source or config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache settings\n",
    "CACHE_DIR = ROOT / 'outputs' / 'cache'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "USE_RAW_CACHE = True\n",
    "RAW_CACHE_PATH = CACHE_DIR / 'raw_df.parquet'\n",
    "USE_PROJ_CACHE = True\n",
    "PROJ_CACHE_PATH = CACHE_DIR / 'projection_df.parquet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418249f8",
   "metadata": {},
   "source": [
    "## Optional: generate a synthetic parquet sample\n",
    "Use this if you want a quick run without connecting to Oracle or preparing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_dataset(num_loans_per_segment: int = 10, max_mob: int = 6) -> pd.DataFrame:\n",
    "    records = []\n",
    "    date_base = pd.Timestamp(\"2024-01-31\")\n",
    "    segments = [(\"A\", \"P1\"), (\"A\", \"P2\"), (\"B\", \"P1\"), (\"B\", \"P2\")]\n",
    "    state_cycle = [\"CURRENT\", \"CURRENT\", \"DPD30+\", \"DPD60+\", \"DPD90+\", \"WRITEOFF\", \"WRITEOFF\"]\n",
    "\n",
    "    for risk_score, product in segments:\n",
    "        for i in range(num_loans_per_segment):\n",
    "            agreement_id = f\"{risk_score}{product}{i}\"\n",
    "            ead0 = 1000 + 50 * i\n",
    "            for mob in range(max_mob + 1):\n",
    "                state = state_cycle[min(mob, len(state_cycle) - 1)]\n",
    "                if mob == max_mob and i % 3 == 1:\n",
    "                    state = \"CLOSED\"\n",
    "                cutoff_date = date_base + pd.DateOffset(months=mob)\n",
    "                ead_value = max(ead0 - mob * 25, 50)\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"AGREEMENT_ID\": agreement_id,\n",
    "                        \"MOB\": mob,\n",
    "                        \"STATE_MODEL\": state,\n",
    "                        \"PRINCIPLE_OUTSTANDING\": float(ead_value),\n",
    "                        \"CUTOFF_DATE\": cutoff_date,\n",
    "                        \"RISK_SCORE\": risk_score,\n",
    "                        \"PRODUCT_TYPE\": product,\n",
    "                    }\n",
    "                )\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "if DATA_SOURCE == \"parquet\" and GENERATE_SYNTHETIC:\n",
    "    PARQUET_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    sample_df = make_synthetic_dataset(num_loans_per_segment=8, max_mob=8)\n",
    "    sample_file = PARQUET_PATH / \"sample.parquet\"\n",
    "    sample_df.to_parquet(sample_file, index=False)\n",
    "    print(\"Synthetic parquet created at\", sample_file)\n",
    "    print(sample_df.head())\n",
    "else:\n",
    "    print(\"Synthetic generation skipped; using existing parquet files at\", PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = default_schema()\n",
    "\n",
    "if USE_RAW_CACHE and RAW_CACHE_PATH.exists():\n",
    "    raw_df = pd.read_parquet(RAW_CACHE_PATH)\n",
    "    print(f'Loaded raw_df from cache: {RAW_CACHE_PATH} shape={raw_df.shape}')\n",
    "else:\n",
    "    raw_df = load_raw_data(schema=schema, source=DATA_SOURCE, parquet_path=PARQUET_PATH)\n",
    "    validate_input(raw_df, schema=schema, max_mob=config.MAX_MOB)\n",
    "    if USE_RAW_CACHE:\n",
    "        RAW_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        raw_df.to_parquet(RAW_CACHE_PATH, index=False)\n",
    "        print(f'Saved raw_df cache to {RAW_CACHE_PATH} shape={raw_df.shape}')\n",
    "\n",
    "raw_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run projection pipeline\n",
    "Use the library runner to build transitions, project EAD, apply calibration, and write outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PROJ_CACHE and PROJ_CACHE_PATH.exists():\n",
    "    projection_df = pd.read_parquet(PROJ_CACHE_PATH)\n",
    "    print(f'Loaded projection from cache: {PROJ_CACHE_PATH} shape={projection_df.shape}')\n",
    "else:\n",
    "    projection_df = run(\n",
    "        asof_date=\"2024-12-31\",\n",
    "        target_mob=config.MAX_MOB,\n",
    "        source=DATA_SOURCE,\n",
    "        parquet_path=PARQUET_PATH,\n",
    "    )\n",
    "    if USE_PROJ_CACHE:\n",
    "        PROJ_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        projection_df.to_parquet(PROJ_CACHE_PATH, index=False)\n",
    "        print(f'Saved projection cache to {PROJ_CACHE_PATH} shape={projection_df.shape}')\n",
    "\n",
    "projection_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect outputs\n",
    "- EAD per state, distribution over EAD0, delinquency indicators\n",
    "- Audit columns: matrix_source, mob_used, n_obs_used, ead_sum_used\n",
    "- Calibration factor (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cols = [col for col in projection_df.columns if col.startswith(\"EAD_\") and col != \"EAD0\"]\n",
    "indicator_cols = [col for col in projection_df.columns if col.startswith(\"DEL_\")]\n",
    "audit_cols = [\"matrix_source\", \"mob_used\", \"n_obs_used\", \"ead_sum_used\", \"calibration_factor\"]\n",
    "display(projection_df[state_cols + indicator_cols + audit_cols].head())\n",
    "\n",
    "# Fallback coverage\n",
    "fallback_rate = (projection_df[\"matrix_source\"] != \"segment_mob\").mean()\n",
    "print(f\"Fallback usage: {fallback_rate*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize delinquency over MOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segment = projection_df[[\"RISK_SCORE\", \"PRODUCT_TYPE\"]].drop_duplicates().iloc[0]\n",
    "mask = (projection_df[\"RISK_SCORE\"] == segment[\"RISK_SCORE\"]) & (projection_df[\"PRODUCT_TYPE\"] == segment[\"PRODUCT_TYPE\"])\n",
    "subset = projection_df.loc[mask]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(subset[\"MOB\"], subset[\"DEL_30P_ON_EAD0\"], label=\"DEL_30P_ON_EAD0\")\n",
    "plt.plot(subset[\"MOB\"], subset[\"DEL_60P_ON_EAD0\"], label=\"DEL_60P_ON_EAD0\")\n",
    "plt.plot(subset[\"MOB\"], subset[\"DEL_90P_ON_EAD0\"], label=\"DEL_90P_ON_EAD0\")\n",
    "plt.xlabel(\"MOB\")\n",
    "plt.ylabel(\"Ratio over EAD0\")\n",
    "plt.title(f\"Delinquency trajectory for segment {segment['RISK_SCORE']} / {segment['PRODUCT_TYPE']}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Excel report\n",
    "output_dir = Path(config.OUTPUT['dir'])\n",
    "report_path = output_dir / config.OUTPUT.get('report_name', 'indicator_report.csv')\n",
    "actual_series = None\n",
    "if report_path.exists():\n",
    "    rep_df = pd.read_csv(report_path)\n",
    "    if 'MOB' in rep_df.columns and 'ACTUAL_DEL30P_ON_EAD0' in rep_df.columns:\n",
    "        actual_series = rep_df.set_index('MOB')['ACTUAL_DEL30P_ON_EAD0']\n",
    "\n",
    "excel_path = output_dir / 'projection_report.xlsx'\n",
    "export_projection_excel(projection_df, output_path=excel_path, actual_del30p_by_mob=actual_series)\n",
    "print('Excel report saved to', excel_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort DEL30 report (Cohort x MOB)\n",
    "T?o b?ng Cohort (th?ng gi?i ng?n) x MOB v?i %DEL30/EAD0 cho actual v? forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = default_schema()\n",
    "cohort_col = getattr(schema, 'cohort_col', None)\n",
    "output_dir = Path(config.OUTPUT['dir'])\n",
    "cohort_report_path = output_dir / config.OUTPUT.get('cohort_report_name', 'cohort_del30_report.csv')\n",
    "cohort_report_excel = output_dir / config.OUTPUT.get('cohort_report_excel_name', 'cohort_del30_report.xlsx')\n",
    "cohort_report_excel_split = output_dir / config.OUTPUT.get('cohort_report_excel_split_name', 'cohort_del30_report_split.xlsx')\n",
    "\n",
    "if cohort_col and cohort_col in raw_df.columns:\n",
    "    export_cohort_del30_report(\n",
    "        raw_df,\n",
    "        projection_df,\n",
    "        output_path=cohort_report_path,\n",
    "        schema=schema,\n",
    "        state_order=config.STATE_ORDER,\n",
    "        buckets_30p=config.BUCKETS_30P,\n",
    "        max_mob=config.MAX_MOB,\n",
    "    )\n",
    "    export_cohort_del30_excel_combined(\n",
    "        raw_df,\n",
    "        projection_df,\n",
    "        output_path=cohort_report_excel,\n",
    "        schema=schema,\n",
    "        state_order=config.STATE_ORDER,\n",
    "        buckets_30p=config.BUCKETS_30P,\n",
    "        max_mob=config.MAX_MOB,\n",
    "    )\n",
    "    export_cohort_del30_excel_split(\n",
    "        raw_df,\n",
    "        projection_df,\n",
    "        output_path=cohort_report_excel_split,\n",
    "        schema=schema,\n",
    "        state_order=config.STATE_ORDER,\n",
    "        buckets_30p=config.BUCKETS_30P,\n",
    "        max_mob=config.MAX_MOB,\n",
    "    )\n",
    "    print('Cohort CSV:', cohort_report_path)\n",
    "    print('Cohort Excel combined:', cohort_report_excel)\n",
    "    print('Cohort Excel split:', cohort_report_excel_split)\n",
    "else:\n",
    "    print('Cohort column not configured or missing; skip cohort report.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecycle Actual + Forecast (demo)\n",
    "Gh?p actual/forecast v? xu?t lifecycle Excel cho t?t c? product/metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo lifecycle export (requires df_actual and df_plan_fc prepared with DPD buckets)\n",
    "# Replace the placeholders with your actual/forecast lifecycle DataFrames.\n",
    "try:\n",
    "    df_actual = df_actual  # provided externally\n",
    "    df_plan_fc = df_plan_fc  # provided externally\n",
    "    buckets = ['DPD30+', 'DPD60+', 'DPD90+']\n",
    "    lifecycle_df = build_lifecycle_for_report(df_actual, df_plan_fc, buckets)\n",
    "    actual_info = {}  # e.g., {('PRODUCT', pd.Timestamp('2024-01-01')): 6}\n",
    "    lc_path = output_dir / 'lifecycle_report.xlsx'\n",
    "    export_lifecycle_all_products_one_file_extended(lifecycle_df, actual_info, lc_path)\n",
    "    print('Lifecycle report saved to', lc_path)\n",
    "except NameError:\n",
    "    print('Define df_actual and df_plan_fc with lifecycle buckets before running this cell.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}